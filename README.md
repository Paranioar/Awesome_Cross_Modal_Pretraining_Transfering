Cross-modal_Retrieval_Tutorial
==============================
The tutorial of Cross-Modal Matching will be constantly updated for Preliminary Insight !

## ``NOTE``
【2023.04.25】Thanks for the continuous attention and emails for follow-up updates. Due to busy schedule, sorry for not updating this project for a long time. 

**At the end of May**, I plan to update more than 100 papers, and open the new section named ***[Transfer Learning]*** including hot *Prompt*, *Adapter*, and so on in Computer Vision, Natural Language Prcessing, Vision-and-Language. So stay tuned!

## ``Logupdate ``

***`` Section Updating ``***

【2021.12.11】 A new section named ***[Video-Text Learning]*** has been added.  
【2021.11.17】 A new section named ***[RSTPReid]*** has been added.  
【2021.08.24】 A new section named ***[ICFG-PEDES]*** has been added.  
【2021.07.10】 A new section named ***[Vision-Language Pretraining]*** has been added.  

***`` Paper Updating ``***

【2022.04.15】 Pretrained works ***[UniT, ViT-BERT, Frozen, PromptFuse, GLUE, CMKT, VLKD, X-VLM, LoopITR, DemoVLP, ViSTA, CoCo-BERT, X-Pool](./method.md/#vision-language-pretraining)*** and Conventional works ***[RELAX, SVSEN, GraDual, CMCAN, ALGCN, DAGNN, DAQN, DLMLG, UARDA, CGMN, TSHSR, VRACR, VSRN++](./method.md/#generic-feature-extraction)*** and Datasets & Surveys ***[ECCVCaption, CxC, Survey](./method.md/#posted-in)*** have been added.   
【2021.12.11】 Pretrained works ***[ActBERT, DeCLIP, TAGS, FILIP, LiT, Florence, OATransformer, RegionLearner](./method.md/#vision-language-pretraining)*** and Conventional works ***[MEMBER, MRL, NCR, LESS, SwAMP, ISERI](./method.md/#generic-feature-extraction)*** have been added.  
【2021.11.17】 Pretrained works ***[TDMR, VLDeformer, CAMoE, MURAL, KD-VLP, DistillVLM, VLMO, ALBEF, SimVLM, CLIP2Video, CLIP2TV](./method.md/#vision-language-pretraining)*** and Conventional works ***[WCGL, LapsCore, AACH, CSCC, Meta-SPN, DSSL, SAM, SSAMT, IMRL](./method.md/#generic-feature-extraction)*** have been added.  
【2021.08.24】 Pretrained works ***[ClipBERT, UC2, LightingDOT, GilBERT, SOHO](./method.md/#vision-language-pretraining)*** and Conventional works ***[OLVSEM, SSP, HEI, SSAN, MGEL, SMFEA, HAN, CAEMCL, A-GANet, CMAAM](./method.md/#generic-feature-extraction)*** have been added.  


## ``Catalogue ``
* [Peformance Comparison](./performance.md)
    * [Flickr8K](./performance.md/#performance-of-flickr8k)
    * [Flickr30K](./performance.md/#performance-of-flickr30k)
    * [MSCOCO1K](./performance.md/#performance-of-mscoco1k)
    * [MSCOCO5K](./performance.md/#performance-of-mscoco5k)
    * [RSTPReid](./performance.md/#performance-of-rstpreid)
    * [CUHK-PEDES](./performance.md/#performance-of-cuhk-pedes)
    * [ICFG-PEDES](./performance.md/#performance-of-icfg-pedes)
    * [CUB-Flowers](./performance.md/#performance-of-cub-flowers)

* [Methods Summary](./method.md)
    * [Vision-Language Pretraining](./method.md/#vision-language-pretraining)
    * [Generic-Feature Extraction](./method.md/#generic-feature-extraction)
    * [Cross-Modal Interaction](./method.md/#cross-modal-interaction)
    * [Similarity Measurement](./method.md/#similarity-measurement)
    * [Commonsense Learning](./method.md/#commonsense-learning)
    * [Adversarial Learning](./method.md/#adversarial-learning)
    * [Loss Function](./method.md/#loss-function)
    * [Un-/Semi-Supervised](./method.md/#un-supervised-or-semi-supervised)
    * [Zero-/Fewer-Shot](./method.md/#zero-shot-or-fewer-shot)
    * [Identification Learning](./method.md/#identification-learning)
    * [Video-Text Learning](https://github.com/danieljf24/awesome-video-text-retrieval)
    * [Scene-Text Learning](./method.md/#scene-text-learning)
    * [Related Works](./method.md/#related-works)  
    * [Posted in](./method.md/#posted-in)
    
* [Other Resources](./resource.md/#other-resources)  
    * [Multimodal Learning](./resource.md/#multimodal-learning)
    * [Graph Learning](./resource.md/#graph-learning)
    * [Fewshot Learning](./resource.md/#fewshot-learning)
    

## ``License ``
[MIT license](LICENSE). If any questions, please contact me at r1228240468@gmail.com.
