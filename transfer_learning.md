Methods Summary of Parameter-Efficient Finetuning
==============================

## ``Catalogue ``
* [Parameter-Efficient Method](#parameter-efficient-method)
* [Memory-Efficient Method](#memory-efficient-method)


### ``*Parameter-Efficient Method*``

**(*ICML2019_Adapter-BERT*) Parameter-Efficient Transfer Learning for NLP.** <br>
*Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly.*<br>
[[paper]](https://arxiv.org/abs/1902.00751)
[[code]](https://github.com/google-research/adapter-bert)

**(*EMNLP2019_Adapter-NMT*) Simple, Scalable Adaptation for Neural Machine Translation.** <br>
*Ankur Bapna, Naveen Arivazhagan, Orhan Firat.*<br>
[[paper]](https://arxiv.org/abs/1909.08478)

**(*arXiv2019_Survey*) A Comprehensive Survey on Transfer Learning.** <br>
*Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He.*<br>
[[paper]](https://arxiv.org/abs/1911.02685)

**(*NeurIPS2020_TinyTL*) TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning.** <br>
*Han Cai, Chuang Gan, Ligeng Zhu, Song Han.*<br>
[[paper]](https://proceedings.neurips.cc/paper/2020/file/81f7acabd411274fcf65ce2070ed568a-Paper.pdf)

**(*EMNLP2020_MAD-X*) MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.** <br>
*Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder.*<br>
[[paper]](https://arxiv.org/abs/2005.00052)
[[code]](https://github.com/Adapter-Hub/adapter-transformers)

**(*EACL2021_AdapterFusion*) AdapterFusion: Non-Destructive Task Composition for Transfer Learning.** <br>
*Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych.*<br>
[[paper]](https://arxiv.org/abs/2005.00247)
[[code]](https://github.com/Adapter-Hub/adapter-transformers)

**(*ACL2021_DiffPruning*) Parameter-Efficient Transfer Learning with Diff Pruning.** <br>
*Demi Guo, Alexander M. Rush, Yoon Kim.*<br>
[[paper]](https://arxiv.org/abs/2012.07463)
[[code]](https://github.com/dguo98/DiffPruning)

**(*arXiv2020_Intrinsic-SAID*) Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning.** <br>
*Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta.*<br>
[[paper]](https://arxiv.org/abs/2012.13255)
[[code]](https://github.com/rabeehk/compacter)

**(*ACL2021_Prefix-Tuning*) Prefix-Tuning: Optimizing Continuous Prompts for Generation.** <br>
*Xiang Lisa Li, Percy Liang.*<br>
[[paper]](https://arxiv.org/abs/2101.00190)

**(*ICML2021_CLIP*) Learning Transferable Visual Models From Natural Language Supervision.** <br>
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.*<br>
[[paper]](https://arxiv.org/abs/2103.00020)
[[code]](https://github.com/OpenAI/CLIP)

**(*ArXiv2021_PHC-GNN*) Parameterized Hypercomplex Graph Neural Networks for Graph Classification.** <br>
*Tuan Le, Marco Bertolini, Frank Noé, Djork-Arné Clevert.*<br>
[[paper]](https://arxiv.org/abs/2103.16584)
[[code]](https://github.com/bayer-science-for-a-better-life/phc-gnn)

**(*EMNLP2021_PEPT*) The Power of Scale for Parameter-Efficient Prompt Tuning.** <br>
*Brian Lester, Rami Al-Rfou, Noah Constant.*<br>
[[paper]](https://arxiv.org/abs/2104.08691)

**(*NeurIPS2021_Compacter*) Compacter: Efficient Low-Rank Hypercomplex Adapter Layers.** <br>
*Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder.*<br>
[[paper]](https://arxiv.org/abs/2106.04647)
[[code]](https://github.com/rabeehk/compacter)

**(*arXiv2021_LoRA*) LoRA: Low-Rank Adaptation of Large Language Models.** <br>
*Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen.*<br>
[[paper]](https://arxiv.org/abs/2106.09685)
[[code]](https://github.com/microsoft/LoRA)

**(*ACL2022_BitFit*) BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models.** <br>
*Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg.*<br>
[[paper]](https://arxiv.org/abs/2106.10199)

**(*NeurIPS2021_Frozen*) Multimodal Few-Shot Learning with Frozen Language Models.** <br>
*Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill.*<br>
[[paper]](https://arxiv.org/abs/2106.13884)

**(*IJCV2022_CoOp*) Learning to Prompt for Vision-Language Models.** <br>
*Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2109.01134)
[[code]](https://github.com/KaiyangZhou/CoOp)

**(*ACL2022_PPT*) PPT: Pre-trained Prompt Tuning for Few-shot Learning.** <br>
*Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang.*<br>
[[paper]](https://arxiv.org/abs/2109.04332)
[[code]](https://github.com/thu-coai/PPT)

**(*arXiv2021_CPT*) CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models.** <br>
*Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun.*<br>
[[paper]](https://arxiv.org/abs/2109.11797)
[[code]](https://github.com/thunlp/CPT)

**(*ICLR2022_UnifiedPET*) Towards a Unified View of Parameter-Efficient Transfer Learning.** <br>
*Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig.*<br>
[[paper]](https://arxiv.org/abs/2110.04366)
[[code]](https://github.com/jxhe/unify-parameter-efficient-tuning)

**(*arXiv2021_CLIP-Adapter*) CLIP-Adapter: Better Vision-Language Models with Feature Adapters.** <br>
*Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2110.04544)
[[code]](https://github.com/gaopengcuhk/CLIP-Adapter)

**(*ACL2022_UniPELT*) UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.** <br>
*Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, Madian Khabsa.*<br>
[[paper]](https://arxiv.org/abs/2110.07577)
[[code]](https://github.com/morningmoni/unipelt)

**(*arXiv2022_LayerNorm-tuning*) How to Adapt Your Large-Scale Vision-and-Language Model for Downstream Image Classification.** <br>
*Konwoo Kim, Michael Laskin, Igor Mordatch, Deepak Pathak.*<br>
[[paper]](https://openreview.net/pdf?id=EhwEUb2ynIa)
[[code]](https://sites.google.com/view/adapt-large-scale-models)

**(*ECCV2022_Tip-Adapter*) Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling.** <br>
*Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li.*<br>
[[paper]](https://arxiv.org/abs/2111.03930)
[[code]](https://github.com/gaopengcuhk/tip-adapter)

**(*NeurIPS2021_FISH-Mask*) Training Neural Networks with Fixed Sparse Masks.** <br>
*Yi-Lin Sung, Varun Nair, Colin Raffel.*<br>
[[paper]](https://arxiv.org/abs/2111.09839)
[[code]](https://github.com/varunnair18/fish)

**(*arXiv2021_BD-ViT*) Benchmarking Detection Transfer Learning with Vision Transformers.** <br>
*Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, Ross Girshick.*<br>
[[paper]](https://arxiv.org/abs/2111.11429)
[[code]](https://github.com/hustvl/mimdet)

**(*CVPR2022_DenseCLIP*) DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting.** <br>
*Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu.*<br>
[[paper]](https://arxiv.org/abs/2112.01518)
[[code]](https://github.com/raoyongming/DenseCLIP)

**(*ECCV2022_Efficient-Prompt*) Prompting Visual-Language Models for Efficient Video Understanding.** <br>
*Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie.*<br>
[[paper]](https://arxiv.org/abs/2112.04478)
[[code]](https://github.com/ju-chen/Efficient-Prompt)

**(*CVPR2022_VL-Adapter*) VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks.** <br>
*Yi-Lin Sung, Jaemin Cho, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2112.06825)
[[code]](https://github.com/ylsung/VL_adapter)

**(*CVPR2022_L2P*) Learning to Prompt for Continual Learning.** <br>
*Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, Tomas Pfister.*<br>
[[paper]](https://arxiv.org/abs/2112.08654)
[[code]](https://github.com/google-research/l2p)

**(*ICML2022_Language-Planners*) Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.** <br>
*Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch.*<br>
[[paper]](https://arxiv.org/abs/2201.07207)
[[code]](https://wenlong.page/language-planner/)

**(*NeurIPS2022_CoT*) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.** <br>
*Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou.*<br>
[[paper]](https://arxiv.org/abs/2201.11903)

**(*arXiv2022_Y-Tuning*) Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning.** <br>
*Yitao Liu, Chenxin An, Xipeng Qiu.*<br>
[[paper]](https://arxiv.org/abs/2202.09817)

**(*CVPR2022_CoCoOp*) Conditional Prompt Learning for Vision-Language Models.** <br>
*Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2203.05557)
[[code]](https://github.com/KaiyangZhou/CoOp)

**(*ECCV2022_VPT*) Visual Prompt Tuning.** <br>
*Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim.*<br>
[[paper]](https://arxiv.org/abs/2203.12119)
[[code]](https://github.com/kmnp/vpt)

**(*ECCV2022_DualPrompt*) DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning.** <br>
*Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, Tomas Pfister.*<br>
[[paper]](https://arxiv.org/abs/2204.04799)
[[code]](https://github.com/google-research/l2p)

**(*arXiv2022_IA3*) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.** <br>
*Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel.*<br>
[[paper]](https://arxiv.org/abs/2205.05638)
[[code]](https://github.com/r-three/t-few)

**(*ICLR2023_ViT-Adapter*) Vision Transformer Adapter for Dense Predictions.** <br>
*Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2205.08534)
[[code]](https://github.com/czczup/vit-adapter)

**(*NeurIPS2022_AdaptFormer*) AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition.** <br>
*Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, Ping Luo.*<br>
[[paper]](https://arxiv.org/abs/2205.13535)
[[code]](https://github.com/ShoufaChen/AdaptFormer)

**(*arXiv2022_NOAH*) Neural Prompt Search.** <br>
*Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2206.04673)
[[code]](https://github.com/ZhangYuanhan-AI/NOAH)

**(*arXiv2022_Convpass*) Convolutional Bypasses Are Better Vision Transformer Adapters.** <br>
*Shibo Jie, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2207.07039)
[[code]](https://github.com/JieShibo/PETL-ViT)

**(*NeurIPS2022_P2P*) P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting.** <br>
*Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu.*<br>
[[paper]](https://arxiv.org/abs/2208.02812)
[[code]](https://github.com/wangzy22/P2P)

**(*NeurIPS2022_PromptGen*) Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models.** <br>
*Chen Henry Wu, Saman Motamed, Shaunak Srivastava, Fernando De la Torre.*<br>
[[paper]](https://arxiv.org/abs/2209.06970)
[[code]](https://github.com/chenwu98/generative-visual-prompt)

**(*NeurIPS2022_ScienceQA*) Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.** <br>
*Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan.*<br>
[[paper]](https://arxiv.org/abs/2209.09513)
[[code]](https://github.com/lupantech/ScienceQA)

**(*arXiv2022_Promptagator*) Promptagator: Few-shot Dense Retrieval From 8 Examples.** <br>
*Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, Ming-Wei Chang.*<br>
[[paper]](https://arxiv.org/abs/2209.11755)

**(*ICLR2023_PromptPG*) Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning.** <br>
*Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan.*<br>
[[paper]](https://arxiv.org/abs/2209.14610)
[[code]](https://github.com/lupantech/PromptPG)

**(*arXiv2022_VPT-GTL*) Visual Prompt Tuning for Generative Transfer Learning.** <br>
*Kihyuk Sohn, Yuan Hao, José Lezama, Luisa Polania, Huiwen Chang, Han Zhang, Irfan Essa, Lu Jiang.*<br>
[[paper]](https://arxiv.org/abs/2210.00990)
[[code]](https://github.com/google-research/generative_transfer)

**(*arXiv2022_LPT*) LPT: Long-tailed Prompt Tuning for Image Classification.** <br>
*Bowen Dong, Pan Zhou, Shuicheng Yan, Wangmeng Zuo.*<br>
[[paper]](https://arxiv.org/abs/2210.01033)

**(*ICLR2023_PLOT*) PLOT: Prompt Learning with Optimal Transport for Vision-Language Models.** <br>
*Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, Kun Zhang.*<br>
[[paper]](https://arxiv.org/abs/2210.01253)
[[code]](https://github.com/CHENGY12/PLOT)

**(*arXiv2022_MaPLe*) MaPLe: Multi-modal Prompt Learning.** <br>
*Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan.*<br>
[[paper]](https://arxiv.org/abs/2210.03117)
[[code]](https://github.com/muzairkhattak/multimodal-prompt-learning)

**(*ICLR2023_Description*) Visual Classification via Description from Large Language Models.** <br>
*Sachit Menon, Carl Vondrick.*<br>
[[paper]](https://arxiv.org/abs/2210.07183)

**(*NeurIPS2022_SSF*) Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning.** <br>
*Dongze Lian, Daquan Zhou, Jiashi Feng, Xinchao Wang.*<br>
[[paper]](https://arxiv.org/abs/2210.08823)
[[code]](https://github.com/dongzelian/ssf)

**(*ICLR2023_reliability*) Prompting GPT-3 To Be Reliable.** <br>
*Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2210.09150)
[[code]](https://github.com/NoviScl/GPT3-Reliability)

**(*CVPR2023_SoLa*) Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks.** <br>
*Hyolim Kang, Hanjung Kim, Joungbin An, Minsu Cho, Seon Joo Kim.*<br>
[[paper]](https://arxiv.org/abs/2211.06023)

**(*CVPR2023_ILM-VP*) Understanding and Improving Visual Prompting: A Label-Mapping Perspective.** <br>
*Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, Sijia Liu.*<br>
[[paper]](https://arxiv.org/abs/2211.11635)
[[code]](https://github.com/OPTML-Group/ILM-VP)

**(*CVPR2023_TaI-DPT*) Texts as Images in Prompt Tuning for Multi-Label Image Recognition.** <br>
*Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo.*<br>
[[paper]](https://arxiv.org/abs/2211.12739)
[[code]](https://github.com/guozix/tai-dpt)

**(*CVPR2023_VoP*) VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval.** <br>
*Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang.*<br>
[[paper]](https://arxiv.org/abs/2211.12764)
[[code]](https://github.com/bighuang624/VoP)

**(*AAAI2023_CLIP-ReID*) CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels.** <br>
*Siyuan Li, Li Sun, Qingli Li.*<br>
[[paper]](https://arxiv.org/abs/2211.13977)
[[code]](https://github.com/Syliz517/CLIP-ReID)

**(*CVPR2023_Painter*) Images Speak in Images: A Generalist Painter for In-Context Visual Learning.** <br>
*Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang.*<br>
[[paper]](https://arxiv.org/abs/2212.02499)
[[code]](https://github.com/baaivision/Painter)

**(*AAAI2023_FacT*) FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer.** <br>
*Shibo Jie, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2212.03145)
[[code]](https://github.com/JieShibo/PETL-ViT)

**(*AAAI2023_VDP*) Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation.** <br>
*Yulu Gan, Yan Bai, Yihang Lou, Xianzheng Ma, Renrui Zhang, Nian Shi, Lin Luo.*<br>
[[paper]](https://arxiv.org/abs/2212.04145)

**(*CVPR2023_PIVOT*) PIVOT: Prompting for Video Continual Learning.** <br>
*Andrés Villa, Juan León Alcázar, Motasem Alfarra, Kumail Alhamoud, Julio Hurtado, Fabian Caba Heilbron, Alvaro Soto, Bernard Ghanem.*<br>
[[paper]](https://arxiv.org/abs/2212.04842)

**(*ACL2023_OFA-PT*) Prompt Tuning for Unified Multimodal Pretrained Models.** <br>
*Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou.*<br>
[[paper]](https://aclanthology.org/2023.findings-acl.27.pdf)
[[code]](https://github.com/OFA-Sys/OFA)

**(*arXiv2023_MM-CoT*) Multimodal Chain-of-Thought Reasoning in Language Models.** <br>
*Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola.*<br>
[[paper]](https://arxiv.org/abs/2302.00923)
[[code]](https://github.com/amazon-science/mm-cot)

**(*ICLR2023_AIM*) AIM: Adapting Image Models for Efficient Video Action Recognition.** <br>
*Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li.*<br>
[[paper]](https://arxiv.org/abs/2302.03024)
[[code]](https://adapt-image-models.github.io/)

**(*arXiv2023_UniAdapter*) UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling.** <br>
*Haoyu Lu, Mingyu Ding, Yuqi Huo, Guoxing Yang, Zhiwu Lu, Masayoshi Tomizuka, Wei Zhan.*<br>
[[paper]](https://arxiv.org/abs/2302.06605)
[[code]](https://github.com/RERV/UniAdapter)

**(*arXiv2023_PTUnifier*) Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts.** <br>
*Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, Xiang Wan.*<br>
[[paper]](https://arxiv.org/abs/2302.08958)
[[code]](https://github.com/zhjohnchan/PTUnifier)

**(*CVPR2023_CaFo*) Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners.** <br>
*Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, Peng Gao.*<br>
[[paper]](https://arxiv.org/abs/2303.02151)
[[code]](https://github.com/ZrrSkywalker/CaFo)

**(*arXiv2023_ComPro*) PLearning Combinatorial Prompts for Universal Controllable Image Captioning.** <br>
*Zhen Wang, Jun Xiao, Yueting Zhuang, Fei Gao, Jian Shao, Long Chen.*<br>
[[paper]](https://arxiv.org/abs/2303.06338)

**(*ICCV2023_SPT*) Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning.** <br>
*Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, Bohan Zhuang.*<br>
[[paper]](https://arxiv.org/abs/2303.08566)
[[code]](https://github.com/ziplab/SPT)

**(*ICCV2023_LAE*) A Unified Continual Learning Framework with General Parameter-Efficient Tuning.** <br>
*Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, Jian Zhang.*<br>
[[paper]](https://arxiv.org/abs/2303.10070)
[[code]](https://github.com/gqk/LAE)

**(*CVPR2023_ViPT*) Visual Prompt Multi-Modal Tracking.** <br>
*Jiawen Zhu, Simiao Lai, Xin Chen, Dong Wang, Huchuan Lu.*<br>
[[paper]](https://arxiv.org/abs/2303.10826)
[[code]](https://github.com/jiawen-zhu/vipt)

**(*arXiv2023_DPL*) Decomposed Prototype Learning for Few-Shot Scene Graph Generation.** <br>
*Xingchen Li, Long Chen, Guikun Chen, Yinfu Feng, Yi Yang, Jun Xiao.*<br>
[[paper]](https://arxiv.org/abs/2303.10863)

**(*CVPR2023_EVP*) Explicit Visual Prompting for Low-Level Structure Segmentations.** <br>
*Weihuang Liu, Xi Shen, Chi-Man Pun, Xiaodong Cun.*<br>
[[paper]](https://arxiv.org/abs/2303.10883)
[[code]](https://github.com/NiFangBaAGe/Explicit-Visual-Prompt)

**(*CVPR2023_SP*) Semantic Prompt for Few-Shot Image Recognition.** <br>
*Wentao Chen, Chenyang Si, Zhang Zhang, Liang Wang, Zilei Wang, Tieniu Tan.*<br>
[[paper]](https://arxiv.org/abs/2303.14123)

**(*ICCV2023_SegGPT*) SegGPT: Segmenting Everything In Context.** <br>
*Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang.*<br>
[[paper]](https://arxiv.org/abs/2304.03284)
[[code]](https://github.com/baaivision/Painter)

**(*CVPR2023_Vita-CLIP*) Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting.** <br>
*Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, Mubarak Shah.*<br>
[[paper]](https://arxiv.org/abs/2304.03307)
[[code]](https://github.com/TalalWasim/Vita-CLIP)

**(*ICCV2023_DiffFit*) DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning.** <br>
*Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, Zhenguo Li.*<br>
[[paper]](https://arxiv.org/abs/2304.06648)
[[code]](https://github.com/vimar-gu/minimaxdiffusion)

**(*EMNLP2023_VL-merging*) An Empirical Study of Multimodal Model Merging.** <br>
*Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2304.14933)
[[code]](https://github.com/ylsung/vl-merging)

**(*NeurIPS2023_Aurora*) Parameter-efficient Tuning of Large-scale Multimodal Foundation Model.** <br>
*Haixin Wang, Xinlong Yang, Jianlong Chang, Dian Jin, Jinan Sun, Shikun Zhang, Xiao Luo, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2305.08381)
[[code]](https://github.com/WillDreamer/Aurora)

**(*arXiv2023_TreePrompt*) TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding.** <br>
*Chenchi Zhang, Jun Xiao, Lei Chen, Jian Shao, Long Chen.*<br>
[[paper]](https://arxiv.org/abs/2305.11497)

**(*arXiv2023_QLoRA*) QLoRA: Efficient Finetuning of Quantized LLMs.** <br>
*Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer.*<br>
[[paper]](https://arxiv.org/abs/2305.14314)
[[code]](https://github.com/artidoro/qlora)

**(*arXiv2023_LOMO*) Full Parameter Fine-tuning for Large Language Models with Limited Resources.** <br>
*Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu.*<br>
[[paper]](https://arxiv.org/abs/2306.09782)
[[code]](https://github.com/openlmlab/lomo)

**(*arXiv2023_APT*) Approximated Prompt Tuning for Vision-Language Pre-trained Models.** <br>
*Qiong Wu, Shubin Huang, Yiyi Zhou, Pingyang Dai, Annan Shu, Guannan Jiang, Rongrong Ji.*<br>
[[paper]](https://arxiv.org/abs/2306.15706)

**(*arXiv2023_LRR*) Look, Remember and Reason: Visual Reasoning with Grounded Rationales.** <br>
*Apratim Bhattacharyya, Sunny Panchal, Mingu Lee, Reza Pourreza, Pulkit Madan, Roland Memisevic.*<br>
[[paper]](https://arxiv.org/abs/2306.17778)

**(*ACMMM2023_Self-PT*) Self-PT: Adaptive Self-Prompt Tuning for Low-Resource Visual Question Answering.** <br>
*Bowen Yuan, Sisi You, Bing-Kun Bao.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3581783.3612222)
[[code]](https://github.com/NJUPT-MCC/Self-PT)

**(*ACMMM2023_VioLET*) VioLET: Vision-Language Efficient Tuning with Collaborative Multi-modal Gradients.** <br>
*Yaoming Wang, Yuchen Liu, Xiaopeng Zhang, Jin Li, Bowen Shi, Chenglin Li, Wenrui Dai, Hongkai Xiong, Qi Tian.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3581783.3611706)
[[code]](https://github.com/Wang-Yaoming/VioLET)

**(*arXiv2023_ReLoRA*) ReLoRA: High-Rank Training Through Low-Rank Updates.** <br>
*Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, Anna Rumshisky.*<br>
[[paper]](https://arxiv.org/abs/2307.05695)
[[code]](https://github.com/guitaricet/relora)

**(*ICCV2023_ETRIS*) Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation.** <br>
*Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, Guanbin Li.*<br>
[[paper]](https://arxiv.org/abs/2307.11545)
[[code]](https://github.com/kkakkkka/ETRIS)

**(*ICCV2023_BI-LoRA*) Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy.** <br>
*Shibo Jie, Haoqing Wang, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2307.16867)
[[code]](https://github.com/jieshibo/petl-vit)

**(*ICCV2023_PromptSwitch*) Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval.** <br>
*Chaorui Deng, Qi Chen, Pengda Qin, Da Chen, Qi Wu.*<br>
[[paper]](https://arxiv.org/abs/2308.07648v1)
[[code]](https://github.com/bladewaltz1/PromptSwitch)

**(*ICCV2023_Tem-adapter*) Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer.** <br>
*Guangyi Chen, Xiao Liu, Guangrun Wang, Kun Zhang, Philip H.S.Torr, Xiao-Ping Zhang, Yansong Tang.*<br>
[[paper]](https://arxiv.org/abs/2308.08414)
[[code]](https://github.com/xliu443/tem-adapter)

**(*ICCV2023_VL-PET*) VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control.** <br>
*Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang.*<br>
[[paper]](https://arxiv.org/abs/2308.09804)
[[code]](https://github.com/HenryHZY/VL-PET)

**(*ICCV2023_VLN-PETL*) VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation.** <br>
*Yanyuan Qiao, Zheng Yu, Qi Wu.*<br>
[[paper]](https://arxiv.org/abs/2308.10172)
[[code]](https://github.com/yanyuanqiao/vln-petl)

**(*NeurIPS2023_DAS*) Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models.** <br>
*Qiong Wu, Wei Yu, Yiyi Zhou, Shubin Huang, Xiaoshuai Sun, Rongrong Ji.*<br>
[[paper]](https://arxiv.org/abs/2309.01479)
[[code]](https://github.com/DoubtedSteam/DAS)

**(*arXiv2023_DePT*) DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.** <br>
*Zhengxiang Shi, Aldo Lipani.*<br>
[[paper]](https://arxiv.org/abs/2309.05173)
[[code]](https://github.com/ZhengxiangShi/DePT)

**(*arXiv2023_Black-Box*) Language Models as Black-Box Optimizers for Vision-Language Models.** <br>
*Shihong Liu, Zhiqiu Lin, Samuel Yu, Ryan Lee, Tiffany Ling, Deepak Pathak, Deva Ramanan.*<br>
[[paper]](https://arxiv.org/abs/2309.05950)
[[code]](https://github.com/shihongl1998/llm-as-a-blackbox-optimizer)

**(*arXiv2023_DePT*) DePT: Decoupled Prompt Tuning.** <br>
*Ji Zhang, Shihan Wu, Lianli Gao, Hengtao Shen, Jingkuan Song.*<br>
[[paper]](https://arxiv.org/abs/2309.07439)
[[code]](https://github.com/Koorye/DePT)

**(*arXiv2023_ECoFLaP*) ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models.** <br>
*Yi-Lin Sung, Jaehong Yoon, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2310.02998)
[[code]](https://ecoflap.github.io/)

**(*CVPR2024_PELA*) PELA: Learning Parameter-Efficient Models with Low-Rank Approximation.** <br>
*Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli.*<br>
[[paper]](https://arxiv.org/abs/2310.10700)
[[code]](https://github.com/guoyang9/pela)

**(*EMNLP2023_Adapters*) Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning.** <br>
*Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engländer, Timo Imhof, Ivan Vulić, Sebastian Ruder, Iryna Gurevych, Jonas Pfeiffer.*<br>
[[paper]](https://arxiv.org/abs/2311.11077)
[[code]](https://github.com/adapter-hub/adapters)

**(*arXiv2023_MultiLoRA*) MultiLoRA: Democratizing LoRA for Better Multi-Task Learning.** <br>
*Yiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang.*<br>
[[paper]](https://arxiv.org/abs/2311.11501)

**(*EMNLP2023_SoRA*) Sparse Low-rank Adaptation of Pre-trained Language Models.** <br>
*Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, Maosong Sun.*<br>
[[paper]](https://arxiv.org/abs/2311.11696)
[[code]](https://github.com/tsinghuac3i/sora)

**(*CVPR2024_SAM-COBOT*) Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model.** <br>
*Zelin Peng, Zhengqin Xu, Zhilin Zeng, Lingxi Xie, Qi Tian, Wei Shen.*<br>
[[paper]](https://arxiv.org/abs/2311.17112)

**(*CVPR2024_GPS*) Gradient-based Parameter Selection for Efficient Fine-Tuning.** <br>
*Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui Zhang, Ekaterina Shutova, Shiji Zhou, Shanghang Zhang.*<br>
[[paper]](https://arxiv.org/abs/2312.10136)

**(*CVPR2024_MV-Adapter*) MV-Adapter: Exploring Parameter Efficient Learning for Video Text Retrieval.** <br>
*Bowen Zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Xueqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen, Jiashi Feng.*<br>
[[paper]](https://arxiv.org/abs/2301.07868)

**(*CVPR2024_ModaVerse*) ModaVerse: Efficiently Transforming Modalities with LLMs.** <br>
*Xinyu Wang, Bohan Zhuang, Qi Wu.*<br>
[[paper]](https://arxiv.org/abs/2401.06395)

**(*CVPR2024_DAPT*) Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis.** <br>
*Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, Xiang Bai.*<br>
[[paper]](https://arxiv.org/abs/2403.01439)
[[code]](https://github.com/LMD0311/DAPT)


### ``*Memory-Efficient Method*``

**(*ECCV2020_Side-Tuning*) Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks.** <br>
*Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, Jitendra Malik.*<br>
[[paper]](https://arxiv.org/abs/1912.13503)
[[code]](https://github.com/jozhang97/side-tuning)

**(*ICML2022_Head2Toe*) Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning.** <br>
*Utku Evci, Vincent Dumoulin, Hugo Larochelle, Michael C. Mozer.*<br>
[[paper]](https://arxiv.org/abs/2201.03529)
[[code]](https://github.com/google-research/head2toe)

**(*NeurIPS2022_LST*) LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning.** <br>
*Yi-Lin Sung, Jaemin Cho, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2206.06522)
[[code]](https://github.com/ylsung/Ladder-Side-Tuning)

**(*CVPR2023_VQT*) Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning.** <br>
*Cheng-Hao Tu, Zheda Mai, Wei-Lun Chao.*<br>
[[paper]](https://arxiv.org/abs/2212.03220)
[[code]](https://github.com/andytu28/VQT)

**(*CVPR2023_SAN*) Side Adapter Network for Open-Vocabulary Semantic Segmentation.** <br>
*Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, Xiang Bai.*<br>
[[paper]](https://arxiv.org/abs/2302.12242)
[[code]](https://github.com/MendelXu/SAN)

**(*NeurIPS2023_InCA*) Your representations are in the network: composable and parallel adaptation for large scale models.** <br>
*Yonatan Dukler, Alessandro Achille, Hao Yang, Varsha Vivek, Luca Zancato, Benjamin Bowman, Avinash Ravichandran, Charless Fowlkes, Ashwin Swaminathan, Stefano Soatto.*<br>
[[paper]](https://arxiv.org/abs/2303.04105)

**(*arXiv2023_E3VA*) Parameter-efficient is not sufficient: Exploring Parameter, Memory, and Time Efficient Adapter Tuning for Dense Predictions.** <br>
*Dongshuo Yin, Xueting Han, Bin Li, Hao Feng, Jing Bai.*<br>
[[paper]](https://arxiv.org/abs/2306.09729)

**(*arXiv2023_HST*) Hierarchical Side-Tuning for Vision Transformers.** <br>
*Weifeng Lin, Ziheng Wu, Jiayu Chen, Wentao Yang, Mingxin Huang, Jun Huang, Lianwen Jin.*<br>
[[paper]](https://arxiv.org/abs/2310.05393)
[[code]](https://github.com/AFeng-x/HST)

**(*NeurIPS2023_Res-Tuning*) Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone.** <br>
*Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou.*<br>
[[paper]](https://arxiv.org/abs/2310.19859)
[[code]](https://res-tuning.github.io/)

**(*arXiv2024_Dr2Net*) Dr2Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning.** <br>
*Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem.*<br>
[[paper]](https://arxiv.org/abs/2401.04105)

**(*arXiv2024_Proxy-Tuning*) Tuning Language Models by Proxy.** <br>
*Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Noah A. Smith.*<br>
[[paper]](https://arxiv.org/abs/2401.08565)

**(*arXiv2024_GaLore*) GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection.** <br>
*Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian.*<br>
[[paper]](https://arxiv.org/abs/2403.03507v1)
[[code]](https://github.com/jiaweizzhao/galore)

**(*CVPR2024_Dr2Net*) Dr2Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning.** <br>
*Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem.*<br>
[[paper]](https://arxiv.org/abs/2401.04105)

**(*CVPR2024_LoSA*) Time-, Memory- and Parameter-Efficient Visual Adaptation.** <br>
*Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab.*<br>
[[paper]](https://arxiv.org/abs/2402.02887)

**(*CVPR2024_UniPT*) UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory.** <br>
*Haiwen Diao, Bo Wan, Ying Zhang, Xu Jia, Huchuan Lu, Long Chen.*<br>
[[paper]](https://arxiv.org/abs/2308.14316)
[[code]](https://github.com/Paranioar/UniPT)