### ``*Parameter-Efficient Finetuning*``

**(*ICML2019_Adapter-BERT*) Parameter-Efficient Transfer Learning for NLP.** <br>
*Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly.*<br>
[[paper]](https://arxiv.org/abs/1902.00751)
[[code]](https://github.com/google-research/adapter-bert)

**(*EMNLP2019_Adapter-NMT*) Simple, Scalable Adaptation for Neural Machine Translation.** <br>
*Ankur Bapna, Naveen Arivazhagan, Orhan Firat.*<br>
[[paper]](https://arxiv.org/abs/1909.08478)

**(*ArXiv2019_Survey*) A Comprehensive Survey on Transfer Learning.** <br>
*Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He.*<br>
[[paper]](https://arxiv.org/abs/1911.02685)

**(*ECCV2020_Side-Tuning*) Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks.** <br>
*Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, Jitendra Malik.*<br>
[[paper]](https://arxiv.org/abs/1912.13503)
[[code]](https://github.com/jozhang97/side-tuning)

**(*NeurIPS2020_TinyTL*) TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning.** <br>
*Han Cai, Chuang Gan, Ligeng Zhu, Song Han.*<br>
[[paper]](https://proceedings.neurips.cc/paper/2020/file/81f7acabd411274fcf65ce2070ed568a-Paper.pdf)

**(*EMNLP2020_MAD-X*) MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.** <br>
*Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder.*<br>
[[paper]](https://arxiv.org/abs/2005.00052)
[[code]](https://github.com/Adapter-Hub/adapter-transformers)

**(*EACL2021_AdapterFusion*) AdapterFusion: Non-Destructive Task Composition for Transfer Learning.** <br>
*Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych.*<br>
[[paper]](https://arxiv.org/abs/2005.00247)
[[code]](https://github.com/Adapter-Hub/adapter-transformers)

**(*ACL2021_DiffPruning*) Parameter-Efficient Transfer Learning with Diff Pruning.** <br>
*Demi Guo, Alexander M. Rush, Yoon Kim.*<br>
[[paper]](https://arxiv.org/abs/2012.07463)
[[code]](https://github.com/dguo98/DiffPruning)

**(*ArXiv2020_Intrinsic-SAID*) Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning.** <br>
*Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta.*<br>
[[paper]](https://arxiv.org/abs/2012.13255)
[[code]](https://github.com/rabeehk/compacter)

**(*ACL2021_Prefix-Tuning*) Prefix-Tuning: Optimizing Continuous Prompts for Generation.** <br>
*Xiang Lisa Li, Percy Liang.*<br>
[[paper]](https://arxiv.org/abs/2101.00190)

**(*ICML2021_CLIP*) Learning Transferable Visual Models From Natural Language Supervision.** <br>
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.*<br>
[[paper]](https://arxiv.org/abs/2103.00020)
[[code]](https://github.com/OpenAI/CLIP)

**(*ArXiv2021_PHC-GNN*) Parameterized Hypercomplex Graph Neural Networks for Graph Classification.** <br>
*Tuan Le, Marco Bertolini, Frank Noé, Djork-Arné Clevert.*<br>
[[paper]](https://arxiv.org/abs/2103.16584)
[[code]](https://github.com/bayer-science-for-a-better-life/phc-gnn)

**(*EMNLP2021_PEPT*) The Power of Scale for Parameter-Efficient Prompt Tuning.** <br>
*Brian Lester, Rami Al-Rfou, Noah Constant.*<br>
[[paper]](https://arxiv.org/abs/2104.08691)

**(*NeurIPS2021_Compacter*) Compacter: Efficient Low-Rank Hypercomplex Adapter Layers.** <br>
*Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder.*<br>
[[paper]](https://arxiv.org/abs/2106.04647)
[[code]](https://github.com/rabeehk/compacter)

**(*ArXiv2021_LoRA*) LoRA: Low-Rank Adaptation of Large Language Models.** <br>
*Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen.*<br>
[[paper]](https://arxiv.org/abs/2106.09685)
[[code]](https://github.com/microsoft/LoRA)

**(*ACL2022_BitFit*) BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models.** <br>
*Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg.*<br>
[[paper]](https://arxiv.org/abs/2106.10199)

**(*NeurIPS2021_Frozen*) Multimodal Few-Shot Learning with Frozen Language Models.** <br>
*Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill.*<br>
[[paper]](https://arxiv.org/abs/2106.13884)

**(*IJCV2022_CoOp*) Learning to Prompt for Vision-Language Models.** <br>
*Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2109.01134)
[[code]](https://github.com/KaiyangZhou/CoOp)

**(*ACL2022_PPT*) PPT: Pre-trained Prompt Tuning for Few-shot Learning.** <br>
*Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang.*<br>
[[paper]](https://arxiv.org/abs/2109.04332)
[[code]](https://github.com/thu-coai/PPT)

**(*ArXiv2021_CPT*) CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models.** <br>
*Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun.*<br>
[[paper]](https://arxiv.org/abs/2109.11797)
[[code]](https://github.com/thunlp/CPT)

**(*ICLR2022_UnifiedPET*) Towards a Unified View of Parameter-Efficient Transfer Learning.** <br>
*Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig.*<br>
[[paper]](https://arxiv.org/abs/2110.04366)
[[code]](https://github.com/jxhe/unify-parameter-efficient-tuning)

**(*ArXiv2021_CLIP-Adapter*) CLIP-Adapter: Better Vision-Language Models with Feature Adapters.** <br>
*Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2110.04544)
[[code]](https://github.com/gaopengcuhk/CLIP-Adapter)

**(*ACL2022_UniPELT*) UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning.** <br>
*Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, Madian Khabsa.*<br>
[[paper]](https://arxiv.org/abs/2110.07577)
[[code]](https://github.com/morningmoni/unipelt)

**(*ArXiv2022_LayerNorm-tuning*) How to Adapt Your Large-Scale Vision-and-Language Model for Downstream Image Classification.** <br>
*Konwoo Kim, Michael Laskin, Igor Mordatch, Deepak Pathak.*<br>
[[paper]](https://openreview.net/pdf?id=EhwEUb2ynIa)
[[code]](https://sites.google.com/view/adapt-large-scale-models)

**(*ECCV2022_Tip-Adapter*) Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling.** <br>
*Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li.*<br>
[[paper]](https://arxiv.org/abs/2111.03930)
[[code]](https://github.com/gaopengcuhk/tip-adapter)

**(*NeurIPS2021_FISH-Mask*) Training Neural Networks with Fixed Sparse Masks.** <br>
*Yi-Lin Sung, Varun Nair, Colin Raffel.*<br>
[[paper]](https://arxiv.org/abs/2111.09839)
[[code]](https://github.com/varunnair18/fish)

**(*CVPR2022_DenseCLIP*) DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting.** <br>
*Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu.*<br>
[[paper]](https://arxiv.org/abs/2112.01518)
[[code]](https://github.com/raoyongming/DenseCLIP)

**(*ECCV2022_Efficient-Prompt*) Prompting Visual-Language Models for Efficient Video Understanding.** <br>
*Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie.*<br>
[[paper]](https://arxiv.org/abs/2112.04478)
[[code]](https://github.com/ju-chen/Efficient-Prompt)

**(*CVPR2022_VL-Adapter*) VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks.** <br>
*Yi-Lin Sung, Jaemin Cho, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2112.06825)
[[code]](https://github.com/ylsung/VL_adapter)

**(*ICML2022_Language-Planners*) Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.** <br>
*Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch.*<br>
[[paper]](https://arxiv.org/abs/2201.07207)
[[code]](https://wenlong.page/language-planner/)

**(*NeurIPS2022_CoT*) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.** <br>
*Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou.*<br>
[[paper]](https://arxiv.org/abs/2201.11903)

**(*ArXiv2022_Y-Tuning*) Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning.** <br>
*Yitao Liu, Chenxin An, Xipeng Qiu.*<br>
[[paper]](https://arxiv.org/abs/2202.09817)

**(*CVPR2022_CoCoOp*) Conditional Prompt Learning for Vision-Language Models.** <br>
*Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2203.05557)
[[code]](https://github.com/KaiyangZhou/CoOp)

**(*ECCV2022_VPT*) Visual Prompt Tuning.** <br>
*Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim.*<br>
[[paper]](https://arxiv.org/abs/2203.12119)
[[code]](https://github.com/kmnp/vpt)

**(*ArXiv2022_IA3*) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning.** <br>
*Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel.*<br>
[[paper]](https://arxiv.org/abs/2205.05638)
[[code]](https://github.com/r-three/t-few)

**(*ICLR2023_ViT-Adapter*) Vision Transformer Adapter for Dense Predictions.** <br>
*Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao.*<br>
[[paper]](https://arxiv.org/abs/2205.08534)
[[code]](https://github.com/czczup/vit-adapter)

**(*NeurIPS2022_AdaptFormer*) AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition.** <br>
*Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, Ping Luo.*<br>
[[paper]](https://arxiv.org/abs/2205.13535)
[[code]](https://github.com/ShoufaChen/AdaptFormer)

**(*ArXiv2022_NOAH*) Neural Prompt Search.** <br>
*Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu.*<br>
[[paper]](https://arxiv.org/abs/2206.04673)
[[code]](https://github.com/ZhangYuanhan-AI/NOAH)

**(*NeurIPS2022_LST*) LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning.** <br>
*Yi-Lin Sung, Jaemin Cho, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2206.06522)
[[code]](https://github.com/ylsung/Ladder-Side-Tuning)

**(*ArXiv2022_Convpass*) Convolutional Bypasses Are Better Vision Transformer Adapters.** <br>
*Shibo Jie, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2207.07039)
[[code]](https://github.com/JieShibo/PETL-ViT)

**(*NeurIPS2022_ScienceQA*) Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.** <br>
*Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan.*<br>
[[paper]](https://arxiv.org/abs/2209.09513)
[[code]](https://github.com/lupantech/ScienceQA)

**(*ICLR2023_PromptPG*) Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning.** <br>
*Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan.*<br>
[[paper]](https://arxiv.org/abs/2209.14610)
[[code]](https://github.com/lupantech/PromptPG)

**(*ArXiv2022_LPT*) LPT: Long-tailed Prompt Tuning for Image Classification.** <br>
*Bowen Dong, Pan Zhou, Shuicheng Yan, Wangmeng Zuo.*<br>
[[paper]](https://arxiv.org/abs/2210.01033)

**(*ICLR2023_PLOT*) PLOT: Prompt Learning with Optimal Transport for Vision-Language Models.** <br>
*Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, Kun Zhang.*<br>
[[paper]](https://arxiv.org/abs/2210.01253)
[[code]](https://github.com/CHENGY12/PLOT)

**(*ArXiv2022_MaPLe*) MaPLe: Multi-modal Prompt Learning.** <br>
*Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan.*<br>
[[paper]](https://arxiv.org/abs/2210.03117)
[[code]](https://github.com/muzairkhattak/multimodal-prompt-learning)

**(*ICLR2023_Description*) Visual Classification via Description from Large Language Models.** <br>
*Sachit Menon, Carl Vondrick.*<br>
[[paper]](https://arxiv.org/abs/2210.07183)

**(*ICLR2023_reliability*) Prompting GPT-3 To Be Reliable.** <br>
*Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2210.09150)
[[code]](https://github.com/NoviScl/GPT3-Reliability)

**(*CVPR2023_SoLa*) Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks.** <br>
*Hyolim Kang, Hanjung Kim, Joungbin An, Minsu Cho, Seon Joo Kim.*<br>
[[paper]](https://arxiv.org/abs/2211.06023)

**(*AAAI2023_CLIP-ReID*) CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels.** <br>
*Siyuan Li, Li Sun, Qingli Li.*<br>
[[paper]](https://arxiv.org/abs/2211.13977)
[[code]](https://github.com/Syliz517/CLIP-ReID)

**(*AAAI2023_FacT*) FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer.** <br>
*Shibo Jie, Zhi-Hong Deng.*<br>
[[paper]](https://arxiv.org/abs/2212.03145)
[[code]](https://github.com/JieShibo/PETL-ViT)

**(*ArXiv2023_MM-CoT*) Multimodal Chain-of-Thought Reasoning in Language Models.** <br>
*Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola.*<br>
[[paper]](https://arxiv.org/abs/2302.00923)
[[code]](https://github.com/amazon-science/mm-cot)

**(*ArXiv2023_UniAdapter*) UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling.** <br>
*Haoyu Lu, Mingyu Ding, Yuqi Huo, Guoxing Yang, Zhiwu Lu, Masayoshi Tomizuka, Wei Zhan.*<br>
[[paper]](https://arxiv.org/abs/2302.06605)
[[code]](https://github.com/RERV/UniAdapter)

**(*ArXiv2023_PTUnifier*) Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts.** <br>
*Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, Xiang Wan.*<br>
[[paper]](https://arxiv.org/abs/2302.08958)
[[code]](https://github.com/zhjohnchan/PTUnifier)

**(*CVPR2023_CaFo*) Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners.** <br>
*Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, Peng Gao.*<br>
[[paper]](https://arxiv.org/abs/2303.02151)
[[code]](https://github.com/ZrrSkywalker/CaFo)
