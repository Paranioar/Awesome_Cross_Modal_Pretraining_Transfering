## ``Vision-Language Pretraining``

### ``*Pretrained Method*``

**(*ICCV2019_VideoBERT*) VideoBERT: A Joint Model for Video and Language Representation Learning.** <br>
*Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/1904.01766)

**(*ICCV2019_HowTo100M*) HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips.** <br>
*Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic.*<br>
[[paper]](https://arxiv.org/abs/1906.03327)
[[code]](http://www.di.ens.fr/willow/research/howto100m/)

**(*arXiv2019_CBT*) Learning Video Representations using Contrastive Bidirectional Transformer.** <br>
*Chen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid.*<br>
[[paper]](https://arxiv.org/abs/1906.05743)

**(*NeurIPS2019_ViLBERT*) ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.** <br>
*Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee.*<br>
[[paper]](https://arxiv.org/abs/1908.02265)
[[code]](https://github.com/jiasenlu/vilbert_beta)

**(*ACL2020_VisualBERT*) VisualBERT: A Simple and Performant Baseline for Vision and Language.** <br>
*Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.*<br>
[[paper]](https://arxiv.org/abs/1908.03557)
[[code]](https://github.com/uclanlp/visualbert)

**(*EMNLP2019_B2T2*) Fusion of Detected Objects in Text for Visual Question Answering.** <br>
*Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter.*<br>
[[paper]](https://arxiv.org/abs/1908.05054)
[[code]](https://github.com/google-research/language/tree/master/language/question_answering/b2t2)

**(*AAAI2020_Unicoder-VL*) Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training.** <br>
*Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Ming Zhou.*<br>
[[paper]](https://arxiv.org/abs/1908.06066)

**(*EMNLP2019_LXMERT*) LXMERT: Learning Cross-Modality Encoder Representations from Transformers.** <br>
*Hao Tan, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/1908.07490)
[[code]](https://github.com/airsplay/lxmert)

**(*ICLR2020_VL-BERT*) VL-BERT: Pre-training of Generic Visual-Linguistic Representations.** <br>
*Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai.*<br>
[[paper]](https://arxiv.org/abs/1908.08530)
[[code]](https://github.com/jackroos/VL-BERT)

**(*AAAI2020_Unified-VLP*) Unified Vision-Language Pre-Training for Image Captioning and VQA.** <br>
*Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/1909.11059)
[[code]](https://github.com/LuoweiZhou/VLP)

**(*ECCV2020_UNITER*) UNITER: UNiversal Image-TExt Representation Learning.** <br>
*Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/1909.11740)
[[code]](https://github.com/ChenRocks/UNITER)

**(*CVPR2020_M4C*) Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA.** <br>
*Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach.*<br>
[[paper]](https://arxiv.org/abs/1911.06258)

**(*CVPR2020_12-in-1*) 12-in-1: Multi-Task Vision and Language Representation Learning.** <br>
*Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee.*<br>
[[paper]](https://arxiv.org/abs/1912.02315)
[[code]](https://github.com/facebookresearch/vilbert-multi-task)

**(*ECCV2020_VisDial-BERT*) Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline.** <br>
*Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das.*<br>
[[paper]](https://arxiv.org/abs/1912.02379)
[[code]](https://github.com/vmurahari3/visdial-bert)

**(*arXiv2020_ImageBERT*) ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data.** <br>
*Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sacheti.*<br>
[[paper]](https://arxiv.org/abs/2001.07966)

**(*arXiv2020_MSB*) Measuring Social Biases in Grounded Vision and Language Embeddings.** <br>
*Candace Ross, Boris Katz, Andrei Barbu.*<br>
[[paper]](https://arxiv.org/abs/2002.08911)
[[code]](https://github.com/candacelax/bias-in-vision-and-language)

**(*CVPR2020_PREVALENT*) Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training.** <br>
*Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2002.10638)
[[code]](https://github.com/weituo12321/PREVALENT)

**(*INLG2020_VQG-BERT*) What BERT Sees: Cross-Modal Transfer for Visual Question Generation.** <br>
*Thomas Scialom, Patrick Bordes, Paul-Alexis Dray, Jacopo Staiano, Patrick Gallinari.*<br>
[[paper]](https://arxiv.org/abs/2002.10832)

**(*arXiv2020_XGPT*) XGPT: Cross-modal Generative Pre-Training for Image Captioning.** <br>
*Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon Bharti, Xin Liu, Ming Zhou.*<br>
[[paper]](https://arxiv.org/abs/2003.01473)

**(*arXiv2020_InterBERT*) InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining.** <br>
*Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, Hongxia Yang.*<br>
[[paper]](https://arxiv.org/abs/2003.13198)

**(*arXiv2020_Pixel-BERT*) Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers.** <br>
*Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu.*<br>
[[paper]](https://arxiv.org/abs/2004.00849)

**(*ECCV2020_Oscar*) Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks.** <br>
*Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2004.06165)
[[code]](https://github.com/microsoft/Oscar)

**(*arXiv2020_MMF*) Are we pretraining it right? Digging deeper into visio-linguistic pretraining.** <br>
*Amanpreet Singh, Vedanuj Goswami, Devi Parikh.*<br>
[[paper]](https://arxiv.org/abs/2004.08744)
[[code]](https://github.com/facebookresearch/mmf)

**(*ACMMM2020_MMNas*) Deep Multimodal Neural Architecture Search.** <br>
*Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2004.12070)
[[code]](https://github.com/MILVLG/mmnas/)

**(*EMNLP2020_VD-BERT*) VD-BERT: A Unified Vision and Dialog Transformer with BERT.** <br>
*Yue Wang, Shafiq Joty, Michael R. Lyu, Irwin King, Caiming Xiong, Steven C.H. Hoi.*<br>
[[paper]](https://arxiv.org/abs/2004.13278)
[[code]](https://github.com/salesforce/VD-BERT)

**(*EMNLP2020_HERO*) HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training.** <br>
*Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2005.00200)
[[code]](https://github.com/linjieli222/HERO)

**(*ECCV2020_VALUE*) Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models.** <br>
*Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2005.07310)
[[code]](https://github.com/JizeCao/VALUE)

**(*ACLSRW2020_AT*) Adaptive Transformers for Learning Multimodal Representations.** <br>
*Prajjwal Bhargava.*<br>
[[paper]](https://arxiv.org/abs/2005.07486)
[[code]](https://github.com/prajjwal1/adaptive_transformer)

**(*NeurIPS2020_VILLA*) Large-Scale Adversarial Training for Vision-and-Language Representation Learning.** <br>
*Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2006.06195)
[[code]](https://github.com/zhegan27/VILLA)

**(*CVPR2021_VirTex*) VirTex: Learning Visual Representations from Textual Annotations.** <br>
*Karan Desai, Justin Johnson.*<br>
[[paper]](https://arxiv.org/abs/2006.06666)
[[code]](https://github.com/kdexd/virtex)

**(*AAAI2021_ERNIE-ViL*) ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph.** <br>
*Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.*<br>
[[paper]](https://arxiv.org/abs/2006.16934)

**(*ACMMM2020_DeVLBert*) DeVLBert: Learning Deconfounded Visio-Linguistic Representations.** <br>
*Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, Fei Wu.*<br>
[[paper]](https://arxiv.org/abs/2008.06884)
[[code]](https://github.com/shengyuzhang/DeVLBert)

**(*Access2021_RVL-BERT*) Visual Relationship Detection With Visual-Linguistic Knowledge From Multimodal Representations.** <br>
*Meng-Jiun Chiou, Roger Zimmermann, Jiashi Feng.*<br>
[[paper]](https://arxiv.org/abs/2009.04965)
[[code]](https://github.com/coldmanck/RVL-BERT)

**(*EMNLP2020_X-LXMERT*) X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers.** <br>
*Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi.*<br>
[[paper]](https://arxiv.org/abs/2009.11278)
[[code]](https://prior.allenai.org/projects/x-lxmert)

**(*arXiv2020_CAPT*) CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations.** <br>
*Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun.*<br>
[[paper]](https://arxiv.org/abs/2010.06351)

**(*EMNLP2020_STL-CQA*) STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering.** <br>
*Hrituraj Singh, Sumit Shekhar.*<br>
[[paper]](https://aclanthology.org/2020.emnlp-main.264.pdf)

**(*CVPR2020_ActBERT*) ActBERT: Learning Global-Local Video-Text Representations.** <br>
*Linchao Zhu, Yi Yang.*<br>
[[paper]](https://arxiv.org/abs/2011.07231)

**(*TACL2021_MPU*) Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs.** <br>
*Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott.*<br>
[[paper]](https://arxiv.org/abs/2011.15124)
[[code]](https://github.com/e-bug/mpre-unmasked)

**(*arXiv2020_LAMP*) LAMP: Label Augmented Multimodal Pretraining.** <br>
*Jia Guo, Chen Zhu, Yilun Zhao, Heda Wang, Yao Hu, Xiaofei He, Deng Cai.*<br>
[[paper]](https://arxiv.org/abs/2012.04446)

**(*arXiv2020_MiniVLM*) MiniVLM: A Smaller and Faster Vision-Language Model.** <br>
*Jianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao, Zicheng Liu.*<br>
[[paper]](https://arxiv.org/abs/2012.06946)

**(*arXiv2020_MANGO*) A Closer Look at the Robustness of Vision-and-Language Pre-trained Models.** <br>
*Linjie Li, Zhe Gan, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2012.08673)

**(*ACL2021_UNIMO*) UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning.** <br>
*Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang.*<br>
[[paper]](https://arxiv.org/abs/2012.15409)
[[code]](https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO)

**(*CVPR2021_VinVL*) VinVL: Revisiting Visual Representations in Vision-Language Models.** <br>
*Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2101.00529)
[[code]](https://github.com/pzzhang/VinVL)

**(*AAAI2021_VisualMRC*) VisualMRC: Machine Reading Comprehension on Document Images.** <br>
*Ryota Tanaka, Kyosuke Nishida, Sen Yoshida.*<br>
[[paper]](https://arxiv.org/abs/2101.11272)
[[code]](https://github.com/nttmdlab-nlp/VisualMRC)

**(*AAAI2021_TDEN*) Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network.** <br>
*Yehao Li, Yingwei Pan, Ting Yao, Jingwen Chen, Tao Mei.*<br>
[[paper]](https://arxiv.org/abs/2101.11562)
[[code]](https://github.com/YehLi/TDEN)

**(*ICML2021_VL-BART*) Unifying Vision-and-Language Tasks via Text Generation.** <br>
*Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2102.02779)
[[code]](https://github.com/j-min/VL-T5)

**(*ICML2021_ViLT*) ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.** <br>
*Wonjae Kim, Bokyung Son, Ildoo Kim.*<br>
[[paper]](https://arxiv.org/abs/2102.03334)
[[code]](https://github.com/dandelin/vilt)

**(*ICML2021_ALIGN*) Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision.** <br>
*Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.*<br>
[[paper]](https://arxiv.org/abs/2102.05918)
[[blog]](https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html)

**(*CVPR2021_ClipBERT*) Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling.** <br>
*Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2102.06183)
[[code]](https://github.com/jayleicn/ClipBERT)

**(*ICCV2021_UniT*) UniT: Multimodal Multitask Learning with a Unified Transformer.** <br>
*Ronghang Hu, Amanpreet Singh.*<br>
[[paper]](https://arxiv.org/abs/2102.10772)
[[code]](https://github.com/facebookresearch/mmf/tree/main/projects/unit)

**(*arXiv2021_CLIP*) Learning Transferable Visual Models From Natural Language Supervision.** <br>
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.*<br>
[[paper]](https://arxiv.org/abs/2103.00020)
[[code]](https://github.com/OpenAI/CLIP)

**(*arXiv2021_SemVLP*) SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels.** <br>
*Chenliang Li, Ming Yan, Haiyang Xu, Fuli Luo, Wei Wang, Bin Bi, Songfang Huang.*<br>
[[paper]](https://arxiv.org/abs/2103.07829)

**(*NAACL2021_LightningDOT*) LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval** <br>
*Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei Fang, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2103.08784)
[[code]](https://github.com/intersun/LightningDOT)

**(*CVPR2021_Fast&Slow*) Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers.** <br>
*Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/2103.16553)

**(*ICCV2021_Frozen*) Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval.** <br>
*Max Bain, Arsha Nagrani, Gül Varol, Andrew Zisserman.*<br>
[[paper]](https://arxiv.org/abs/2104.00650)
[[code]](https://github.com/m-bain/frozen-in-time)

**(*CVPR2021_UC2*) UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pre-training.** <br>
*Mingyang Zhou, Luowei Zhou, Shuohang Wang, Yu Cheng, Linjie Li, Zhou Yu, Jingjing Liu.*<br>
[[paper]](https://arxiv.org/abs/2104.00332)

**(*ICCV2021_DistillVLM*) Compressing Visual-linguistic Model via Knowledge Distillation.** <br>
*Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, Zicheng Liu.*<br>
[[paper]](https://arxiv.org/abs/2104.02096)

**(*CVPR2021_SOHO*) Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning.** <br>
*Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu.*<br>
[[paper]](https://arxiv.org/abs/2104.03135)
[[code]](https://github.com/researchmm/soho)

**(*EMNLP2021_GLUE*) Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models.** <br>
*Taichi Iki, Akiko Aizawa.*<br>
[[paper]](https://arxiv.org/abs/2104.08066)
[[code]](https://github.com/alab-nii/eval_vl_glue)

**(*ICCV2021_TEACHTEXT*) TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval.** <br>
*Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, Yang Liu.*<br>
[[paper]](https://arxiv.org/abs/2104.08271)
[[code]](https://github.com/albanie/collaborative-experts)

**(*Neurocomputing2022_CLIP4Clip*) CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval.** <br>
*Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, Tianrui Li.*<br>
[[paper]](https://arxiv.org/abs/2104.08860)
[[code]](https://github.com/ArrowLuo/CLIP4Clip)

**(*ICCV2021_MDETR*) MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding.** <br>
*Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, Nicolas Carion.*<br>
[[paper]](https://arxiv.org/abs/2104.12763)
[[code]](https://github.com/ashkamath/mdetr)

**(*CVPR2021_MCT*) Multimodal Contrastive Training for Visual Representation Learning.** <br>
*Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, Baldo Faieta.*<br>
[[paper]](https://arxiv.org/abs/2104.12836)

**(*ACLIJCNLP2021_IAIS*) Learning Relation Alignment for Calibrated Cross-modal Retrieval.** <br>
*Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren Zhou, Xu Sun, Hongxia Yang.*<br>
[[paper]](https://arxiv.org/abs/2105.13868)
[[code]](https://github.com/lancopku/IAIS)

**(*arXiv2021_CLIP2Video*) CLIP2Video: Mastering Video-Text Retrieval via Image CLIP.** <br>
*Han Fang, Pengfei Xiong, Luhui Xu, Yu Chen.*<br>
[[paper]](https://arxiv.org/abs/2106.11097)
[[code]](https://github.com/CryhanFang/CLIP2Video)

**(*ICLR2022_CLIP-ViL*) How Much Can CLIP Benefit Vision-and-Language Tasks?.** <br>
*Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer.*<br>
[[paper]](https://arxiv.org/abs/2107.06383)
[[code]](https://github.com/clip-vil/CLIP-ViL)

**(*SIGIR2021_GilBERT*) GilBERT: Generative Vision-Language Pre-Training for Image-Text Retrieval.** <br>
*Weixiang Hong, Kaixiang Ji, Jiajia Liu, Jian Wang, Jingdong Chen, Wei Chu.*<br>
[[paper]](https://dl.acm.org/doi/pdf/10.1145/3404835.3462838)

**(*NeurIPS2021_ALBEF*) Align before Fuse: Vision and Language Representation Learning with Momentum Distillation.** <br>
*Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, Steven Hoi.*<br>
[[paper]](https://arxiv.org/abs/2107.07651)
[[code]](https://github.com/salesforce/ALBEF)

**(*NeurIPS2021_Frozen*) Multimodal Few-Shot Learning with Frozen Language Models.** <br>
*Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill.*<br>
[[paper]](https://openreview.net/pdf?id=WtmMyno9Tq2)
[[project]](https://fh295.github.io/frozen.html)

**(*arXiv2021_SimVLM*) SimVLM: Simple Visual Language Model Pretraining with Weak Supervision.** <br>
*Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao.*<br>
[[paper]](https://arxiv.org/abs/2108.10904)

**(*arXiv2021_CAMoE*) Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss.** <br>
*Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, Dong Shen.*<br>
[[paper]](https://arxiv.org/abs/2109.04290)

**(*arXiv2021_MURAL*) MURAL: Multimodal, Multitask Retrieval Across Languages.** <br>
*Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, Jason Baldridge.*<br>
[[paper]](https://arxiv.org/abs/2109.05125)

**(*arXiv2021_KD-VLP*) KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation.** <br>
*Yongfei Liu, Chenfei Wu, Shao-yen Tseng, Vasudev Lal, Xuming He, Nan Duan.*<br>
[[paper]](https://arxiv.org/abs/2109.10504)

**(*CIKM2021_TDMR*) Student Can Also be a Good Teacher: Extracting Knowledge from Vision-and-Language Model for Cross-Modal Retrieval.** <br>
*Jun Rao, Tao Qian, Shuhan Qi, Yulin Wu, Qing Liao, Xuan Wang.*<br>
[[paper]](https://dl.acm.org/doi/pdf/10.1145/3459637.3482194)

**(*ICCV2021_COOKIE*) COOKIE: Contrastive Cross-Modal Knowledge Sharing Pre-Training for Vision-Language Representation.** <br>
*Keyu Wen, Jin Xia, Yuanyuan Huang, Linyang Li, Jiayan Xu, Jie Shao.*<br>
[[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wen_COOKIE_Contrastive_Cross-Modal_Knowledge_Sharing_Pre-Training_for_Vision-Language_Representation_ICCV_2021_paper.pdf)
[[code]](https://github.com/kywen1119/COOKIE)

**(*arXiv2021_DeCLIP*) Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm.** <br>
*Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, Junjie Yan.*<br>
[[paper]](https://arxiv.org/abs/2110.05208)

**(*arXiv2021_VLDeformer*) VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing.** <br>
*Lisai Zhang, Hongfa Wu, Qingcai Chen, Yimeng Deng, Zhonghua Li, Dejiang Kong, Zhao Cao, Joanna Siebert, Yunpeng Han.*<br>
[[paper]](https://arxiv.org/abs/2110.11338)

**(*arXiv2021_VLMo*) VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts.** <br>
*Wenhui Wang, Hangbo Bao, Li Dong, Furu Wei.*<br>
[[paper]](https://arxiv.org/abs/2111.02358)
[[code]](https://github.com/microsoft/unilm)

**(*CVPR2022_METER*) An Empirical Study of Training End-to-End Vision-and-Language Transformers.** <br>
*Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, Michael Zeng.*<br>
[[paper]](https://arxiv.org/abs/2111.02387)
[[code]](https://github.com/zdou0830/METER)

**(*arXiv2021_TAGS*) Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval.** <br>
*Zhihao Fan, Zhongyu Wei, Zejun Li, Siyuan Wang, Jianqing Fan.*<br>
[[paper]](https://arxiv.org/abs/2111.03349)
[[code]](https://github.com/LibertFan/TAGS)

**(*arXiv2021_CLIP2TV*) CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval.** <br>
*Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, Jinwei Yuan.*<br>
[[paper]](https://arxiv.org/abs/2111.05610)

**(*arXiv2021_FILIP*) FILIP: Fine-grained Interactive Language-Image Pre-Training.** <br>
*Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu.*<br>
[[paper]](https://arxiv.org/abs/2111.07783)

**(*arXiv2021_LiT*) LiT: Zero-Shot Transfer with Locked-image Text Tuning.** <br>
*Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer.*<br>
[[paper]](https://arxiv.org/abs/2111.07991)

**(*arXiv2021_X-VLM*) Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts.** <br>
*Yan Zeng, Xinsong Zhang, Hang Li.*<br>
[[paper]](https://arxiv.org/abs/2111.08276)
[[code]](https://github.com/zengyan-97/x-vlm)

**(*arXiv2021_Florence*) Florence: A New Foundation Model for Computer Vision.** <br>
*Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang.*<br>
[[paper]](https://arxiv.org/abs/2111.11432)

**(*arXiv2021_OA-Transformer*) Object-aware Video-language Pre-training for Retrieval.** <br>
*Alex Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2112.00656)
[[code]](https://github.com/FingerRec/OA-Transformer)

**(*arXiv2021_RegionLearner*) Video-Text Pre-training with Learned Regions.** <br>
*Rui Yan, Mike Zheng Shou, Yixiao Ge, Alex Jinpeng Wang, Xudong Lin, Guanyu Cai, Jinhui Tang.*<br>
[[paper]](https://arxiv.org/abs/2112.01194)
[[code]](https://github.com/ruiyan1995/Region_Learner)

**(*CVPR2022_GLIP*) Grounded Language-Image Pre-training.** <br>
*Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2112.03857)
[[code]](https://github.com/microsoft/GLIP)

**(*arXiv2021_ViT-BERT*) Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text.** <br>
*Qing Li, Boqing Gong, Yin Cui, Dan Kondratyuk, Xianzhi Du, Ming-Hsuan Yang, Matthew Brown.*<br>
[[paper]](https://arxiv.org/abs/2112.07074)

**(*ACMMM2021_CoCo-BERT*) CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising.** <br>
*Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Hongyang Chao, Tao Mei.*<br>
[[paper]](https://arxiv.org/abs/2112.07515)

**(*arXiv2021_SLIP*) SLIP: Self-supervision meets Language-Image Pre-training.** <br>
*Norman Mu, Alexander Kirillov, David Wagner, Saining Xie.*<br>
[[paper]](https://arxiv.org/abs/2112.12750)
[[code]](https://github.com/facebookresearch/slip)

**(*CVPR2022_QB-NORM*) Cross Modal Retrieval with Querybank Normalisation.** <br>
*Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu, Samuel Albanie.*<br>
[[paper]](https://arxiv.org/abs/2112.12777)
[[code]](https://github.com/ioanacroi/qb-norm)

**(*ACLARR_PromptFuse*) Prompting as Multimodal Fusing.** <br>
[[paper]](https://openreview.net/pdf?id=wWZCNLkK-FK)

**(*TCSVT2022_CSIC*) Image-Text Retrieval with Cross-Modal Semantic Importance Consistency.** <br>
*Zejun Liu, Fanglin Chen, Jun Xu, Wenjie Pei, Guangming Lu.*<br>
[[paper]](https://ieeexplore.ieee.org/abstract/document/9940913)

**(*PMLR2022_VLUE*) VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models.** <br>
*Wangchunshu Zhou, Yan Zeng, Shizhe Diao, Xinsong Zhang.*<br>
[[paper]](https://proceedings.mlr.press/v162/zhou22n/zhou22n.pdf)
[[code]](https://github.com/MichaelZhouwang/VLUE)

**(*CVPR2022_MCQ*) Bridging Video-text Retrieval with Multiple Choice Questions.** <br>
*Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, Ping Luo.*<br>
[[paper]](https://arxiv.org/abs/2201.04850)
[[code]](https://github.com/TencentARC/MCQ)

**(*CVPR2022_TCL*) Vision-Language Pre-Training with Triple Contrastive Learning.** <br>
*Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, Junzhou Huang.*<br>
[[paper]](https://arxiv.org/abs/2202.10401v3)
[[code]](https://github.com/uta-smile/TCL)

**(*CVPR2022_CODIS*) Multi-modal Alignment using Representation Codebook.** <br>
*Jiali Duan, Liqun Chen, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi.*<br>
[[paper]](https://arxiv.org/abs/2203.00048)

**(*arXiv2022_LoopITR*) LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval.** <br>
*Jie Lei, Xinlei Chen, Ning Zhang, Mengjiao Wang, Mohit Bansal, Tamara L. Berg, Licheng Yu.*<br>
[[paper]](https://arxiv.org/abs/2203.05465)

**(*ACL2022_VLKD*) Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation.** <br>
*Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung.*<br>
[[paper]](https://arxiv.org/abs/2203.06386)

**(*arXiv2022_MDMMT-2*) MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization.** <br>
*Alexander Kunitsyn, Maksim Kalashnikov, Maksim Dzabraev, Andrei Ivaniuta.*<br>
[[paper]](https://arxiv.org/abs/2203.07086)

**(*arXiv2022_DRL*) Disentangled Representation Learning for Text-Video Retrieval.** <br>
*Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, Xian-Sheng Hua.*<br>
[[paper]](https://arxiv.org/abs/2203.07111)
[[code]](https://github.com/foolwood/DRL)

**(*ACL2022_CMKT*) Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer.** <br>
*Woojeong Jin, Dong-Ho Lee, Chenguang Zhu, Jay Pujara, Xiang Ren.*<br>
[[paper]](https://arxiv.org/abs/2203.07519)
[[code]](https://github.com/INK-USC/CMKT)

**(*arXiv2022_DemoVLP*) Revitalize Region Feature for Democratizing Video-Language Pre-training.** <br>
*Guanyu Cai, Yixiao Ge, Alex Jinpeng Wang, Rui Yan, Xudong Lin, Ying Shan, Lianghua He, Xiaohu Qie, Jianping Wu, Mike Zheng Shou.*<br>
[[paper]](https://arxiv.org/abs/2203.07720)
[[code]](https://github.com/showlab/DemoVLP)

**(*CVPR2022_X-Pool*) X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval.** <br>
*Satya Krishna Gorti, Noel Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, Guangwei Yu.*<br>
[[paper]](https://arxiv.org/abs/2203.15086)
[[code]](https://github.com/layer6ai-labs/xpool)
[[project]](https://layer6ai-labs.github.io/xpool/)

**(*CVPR2022_ViSTA*) ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval.** <br>
*Mengjun Cheng, Yipeng Sun, Longchao Wang, Xiongwei Zhu, Kun Yao, Jie Chen, Guoli Song, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang.*<br>
[[paper]](https://arxiv.org/abs/2203.16778)

**(*CVPR2022_UniCL*) Unified Contrastive Learning in Image-Text-Label Space.** <br>
*Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao.*<br>
[[paper]](https://arxiv.org/abs/2204.03610)
[[code]](https://github.com/microsoft/UniCL)

**(*CVPR2022_PSD*) Robust Cross-Modal Representation Learning with Progressive Self-Distillation.** <br>
*Alex Andonian, Shixing Chen, Raffay Hamid.*<br>
[[paper]](https://arxiv.org/abs/2204.04588)

**(*CVPR2022_COTS*) COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval.** <br>
*Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, Ji-Rong Wen.*<br>
[[paper]](https://arxiv.org/abs/2204.07441)

**(*CVPR2022_MILES*) MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval.** <br>
*Yuying Ge, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo.*<br>
[[paper]](https://arxiv.org/abs/2204.12408)
[[code]](https://github.com/tencentarc/mcq)

**(*TMLR2023_LTD*) Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval.** <br>
*Maurits Bleeker, Andrew Yates, Maarten de Rijke.*<br>
[[paper]](https://arxiv.org/abs/2204.13382)
[[code]](https://github.com/mauritsbleeker/reducing-predictive-feature-suppression)

**(*arXiv2022_PyramidCLIP*) PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining.** <br>
*Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, Chunhua Shen.*<br>
[[paper]](https://arxiv.org/abs/2204.14095)

**(*SIGIR2022_CenterCLIP*) CenterCLIP: Token Clustering for Efficient Text-Video Retrieval.** <br>
*Shuai Zhao, Linchao Zhu, Xiaohan Wang, Yi Yang.*<br>
[[paper]](https://arxiv.org/abs/2205.00823)
[[code]](https://github.com/mzhaoshuai/CenterCLIP)

**(*arXiv2022_HiVLP*) HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval.** <br>
*Feilong Chen, Xiuyi Chen, Jiaxin Shi, Duzhen Zhang, Jianlong Chang, Qi Tian.*<br>
[[paper]](https://arxiv.org/abs/2205.12105)

**(*arXiv2022_COOKIE*) Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval.** <br>
*Keyu Wen, Zhenshan Tan, Qingrong Cheng, Cheng Chen, Xiaodong Gu.*<br>
[[paper]](https://arxiv.org/abs/2207.00733)

**(*SIGIR2022_CRET*) CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval.** <br>
*Kaixiang Ji, Jiajia Liu, Weixiang Hong, Liheng Zhong, Jian Wang, Jingdong Chen, Wei Chu.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3477495.3531960)

**(*ECCV2022_LocVTP*) LocVTP: Video-Text Pre-training for Temporal Localization.** <br>
*Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang, Yuexian Zou.*<br>
[[paper]](https://arxiv.org/abs/2207.10362)
[[code]](https://github.com/mengcaopku/LocVTP)

**(*CBMI2022_ALADIN*) ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval.** <br>
*Nicola Messina, Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, Giuseppe Amato, Rita Cucchiara.*<br>
[[paper]](https://arxiv.org/abs/2207.14757)
[[code]](https://github.com/mesnico/ALADIN)

**(*NeurIPS2022_LOUPE*) Fine-Grained Semantically Aligned Vision-Language Pre-Training.** <br>
*Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, Siliang Tang.*<br>
[[paper]](https://arxiv.org/abs/2208.02515)
[[code]](https://github.com/yyjmjc/loupe)

**(*ECCV2022_GRIT-VLP*) GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training.** <br>
*Jaeseok Byun, Taebaek Hwang, Jianlong Fu, Taesup Moon.*<br>
[[paper]](https://arxiv.org/abs/2208.04060)
[[code]](https://github.com/jaeseokbyun/GRIT-VLP)

**(*arXiv2022_TokenFlow*) TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval.** <br>
*Xiaohan Zou, Changqiao Wu, Lele Cheng, Zhongyuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2209.13822)

**(*NeurIPS2022_Knowledge-CLIP*) Contrastive Language-Image Pre-Training with Knowledge Graphs.** <br>
*Xuran Pan, Tianzhu Ye, Dongchen Han, Shiji Song, Gao Huang.*<br>
[[paper]](https://arxiv.org/abs/2210.08901)

**(*arXiv2022_xCLIP*) Non-Contrastive Learning Meets Language-Image Pre-Training.** <br>
*Jinghao Zhou, Li Dong, Zhe Gan, Lijuan Wang, Furu Wei.*<br>
[[paper]](https://arxiv.org/abs/2210.09304)

**(*BMVC2022_ViCHA*) Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment.** <br>
*Mustafa Shukor, Guillaume Couairon, Matthieu Cord.*<br>
[[paper]](https://hal.science/hal-03811336v1/file/Long_version_arxiv.pdf)
[[code]](https://github.com/mshukor/ViCHA)

**(*ACMMM2022_CMAL*) CMAL: A Novel Cross-Modal Associative Learning Framework for Vision-Language Pre-Training.** <br>
*Zhiyuan Ma, Jianjun Li, Guohui Li, Kaiyan Huang.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3503161.3548292?casa_token=hi-6v_U02LUAAAAA%3A9bC8zagp-Strds7Ik1iST5VWvADRENtFa_R_vh2gBKfjxyTg1o-7LxTFhM16Q00mdg6l_7-vaWQOqNE)

**(*ACMMM2022_MVPTR*) MVPTR: Multi-Level Semantic Alignment for Vision-Language Pre-Training via Multi-Stage Learning.** <br>
*Zejun Li, Zhihao Fan, Huaixiao Tou, Jingjing Chen, Zhongyu Wei, Xuanjing Huang.*<br>
[[paper]](https://dl.acm.org/doi/abs/10.1145/3503161.3548341?casa_token=VWAkYtj9sQ8AAAAA%3A76l66PyWE1IXcXSHMTXzVndjPS61VGwPpd1RgCid8oKoJJ-f_6qtIDDmDzG16TVj20GUjTAPkElk53Q)

**(*CVPR2022_CLIP-Event*) CLIP-Event: Connecting Text and Images with Event Structures.** <br>
*Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, Shih-Fu Chang.*<br>
[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf)
[[code]](https://github.com/limanling/clip-event)

**(*CVPR2023_TCL*) Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs.** <br>
*Junbum Cha, Jonghwan Mun, Byungseok Roh.*<br>
[[paper]](https://arxiv.org/abs/2212.00785)
[[code]](https://github.com/kakaobrain/tcl)

**(*arXiv2022_MAC*) Masked Contrastive Pre-Training for Efficient Video-Text Retrieval.** <br>
*Fangxun Shu, Biaolong Chen, Yue Liao, Shuwen Xiao, Wenyu Sun, Xiaobo Li, Yousong Zhu, Jinqiao Wang, Si Liu.*<br>
[[paper]](https://arxiv.org/abs/2212.00986)
[[code]](https://github.com/shufangxun/MAC)

**(*AAAI2023_NLIP*) NLIP: Noise-robust Language-Image Pre-training.** <br>
*Runhui Huang, Yanxin Long, Jianhua Han, Hang Xu, Xiwen Liang, Chunjing Xu, Xiaodan Liang.*<br>
[[paper]](https://arxiv.org/abs/2212.07086)

**(*CVPR2023_BIKE*) Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models.** <br>
*Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang.*<br>
[[paper]](https://arxiv.org/abs/2301.00182)
[[code]](https://github.com/whwu95/BIKE)

**(*CVPR2023_Cap4Video*) Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?.** <br>
*Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, Wanli Ouyang.*<br>
[[paper]](https://arxiv.org/abs/2301.00184)
[[code]](https://github.com/whwu95/Cap4Video)

**(*arXiv2023_HADA*) HADA: A Graph-based Amalgamation Framework in Image-text Retrieval.** <br>
*Manh-Duy Nguyen, Binh T. Nguyen, Cathal Gurrin.*<br>
[[paper]](https://arxiv.org/abs/2301.04742)
[[code]](https://github.com/m2man/HADA)

**(*ICCV2023_LexLIP*) LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval.** <br>
*Ziyang luo, Pu Zhao, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Jing Ma, Qingwen lin, Daxin Jiang.*<br>
[[paper]](https://arxiv.org/abs/2302.02908)
[[code]](https://github.com/chiyeunglaw/lexlip-iccv23)

**(*arXiv2023_VITR*) VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-Modal Information Retrieval.** <br>
*Yan Gong, Georgina Cosma, Axel Finke.*<br>
[[paper]](https://arxiv.org/abs/2302.06350)

**(*arXiv2023_UKnow*) UKnow: A Unified Knowledge Protocol for Common-Sense Reasoning and Vision-Language Pre-training.** <br>
*Biao Gong, Xiaoying Xie, Yutong Feng, Yiliang Lv, Yujun Shen, Deli Zhao.*<br>
[[paper]](https://arxiv.org/abs/2302.06891)
[[code]](https://github.com/Gongggg/UKnow)

**(*AAAI2023_STOA-VLP*) STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training.** <br>
*Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng Gong, Xiaocheng Feng, Bing Qin.*<br>
[[paper]](https://arxiv.org/abs/2302.09736)

**(*arXiv2023_CAVL*) CAVL: Learning Contrastive and Adaptive Representations of Vision and Language.** <br>
*Shentong Mo, Jingfei Xia, Ihor Markevych.*<br>
[[paper]](https://arxiv.org/abs/2304.04399)

**(*ICML2023_MERU*) Hyperbolic Image-Text Representations.** <br>
*Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, Ramakrishna Vedantam.*<br>
[[paper]](https://arxiv.org/abs/2304.09172)
[[code]](https://github.com/facebookresearch/meru)

**(*ACL2023_MI*) Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation.** <br>
*Chaoya Jiang, Wei Ye, Haiyang Xu, Miang yan, Shikun Zhang, Jie Zhang, Fei Huang.*<br>
[[paper]](https://arxiv.org/abs/2305.04474)

**(*ACMMM2023_COPA*) COPA: Efficient Vision-Language Pre-training Through Collaborative Object- and Patch-Text Alignment.** <br>
*Chaoya Jiang, Haiyang Xu, Wei Ye, Qinghao Ye, Chenliang Li, Ming Yan, Bin Bi, Shikun Zhang, Ji Zhang, Fei Huang.*<br>
[[paper]](https://arxiv.org/abs/2308.03475)

**(*ICCV2023_UCoFiA*) Unified Coarse-to-Fine Alignment for Video-Text Retrieval.** <br>
*Ziyang Wang, Yi-Lin Sung, Feng Cheng, Gedas Bertasius, Mohit Bansal.*<br>
[[paper]](https://arxiv.org/abs/2309.10091)
[[code]](https://github.com/Ziyang412/UCoFiA)


### ``*Pretrained Dataset*``

**(*SIGIR2023_COCO-F30K-FG*) Rethinking Benchmarks for Cross-modal Image-text Retrieval.** <br>
*Weijing Chen, Linli Yao, Qin Jin.*<br>
[[paper]](https://arxiv.org/abs/2304.10824)
[[code]](https://github.com/cwj1412/MSCOCO-Flikcr30K_FG)

**(*NIPS2011_SBU*) Im2Text: Describing Images Using 1 Million Captioned Photographs.** <br>
*Vicente Ordonez, Girish Kulkarni, Tamara Berg.*<br>
[[paper]](https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf)

**(*arXiv2015_YFCC100M*) YFCC100M: The New Data in Multimedia Research.** <br>
*Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, Li-Jia Li.*<br>
[[paper]](https://arxiv.org/abs/1503.01817v2)

**(*IJCV2017_VG*) Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.** <br>
*Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li.*<br>
[[paper]](https://arxiv.org/abs/1602.07332)

**(*ICCV2017_JFT-300M*) Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.** <br>
*Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta.*<br>
[[paper]](https://arxiv.org/abs/1707.02968v2)

**(*SIGIR2021_WIT*) WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning.** <br>
*Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, Marc Najork.*<br>
[[paper]](https://arxiv.org/abs/2103.01913)
[[code]](https://github.com/google-research-datasets/wit)

**(*CVPR2021_CC-12M*) Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts.** <br>
*Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut.*<br>
[[paper]](https://arxiv.org/abs/2102.08981)
[[code]](https://github.com/google-research-datasets/conceptual-12m)

**(*NeurIPS2021_RedCaps*) RedCaps: web-curated image-text data created by the people, for the people.** <br>
*Karan Desai, Gaurav Kaul, Zubin Aysola, Justin Johnson.*<br>
[[paper]](https://arxiv.org/abs/2111.11431)
[[code]](https://redcaps.xyz)

**(*CVPR2022_LiT*) LiT: Zero-Shot Transfer with Locked-image text Tuning.** <br>
*Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer.*<br>
[[paper]](https://arxiv.org/abs/2111.07991)
[[code]](https://github.com/google-research/vision_transformer#lit-models)

**(*CVPR2022_ALT200M*) Scaling Up Vision-Language Pre-training for Image Captioning.** <br>
*Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2111.12233)
[[code]](https://github.com/xiaoweihu/ALT200M)

**(*TMLR2022_GIT*) GIT: A Generative Image-to-text Transformer for Vision and Language.** <br>
*Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.*<br>
[[paper]](https://arxiv.org/abs/2205.14100)
[[code]](https://github.com/microsoft/GenerativeImage2Text)

**(*ICLR2023_WebLI*) PaLI: A Jointly-Scaled Multilingual Language-Image Model.** <br>
*Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut.*<br>
[[paper]](https://arxiv.org/abs/2209.06794)

**(*NeurIPS2022_LAION-5B*) LAION-5B: An open large-scale dataset for training next generation image-text models.** <br>
*Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev.*<br>
[[paper]](https://arxiv.org/abs/2210.08402)
[[code]](https://github.com/mlfoundations/open_clip)

**(*Github2022_COYO-700M*) COYO-700M: Image-Text Pair Dataset.** <br>
*Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon.*<br>
[[code]](https://github.com/kakaobrain/coyo-dataset)
